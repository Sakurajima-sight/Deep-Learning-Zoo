import streamlit as st
import torch
from transformers import BertTokenizer, BertForQuestionAnswering
from model import BERT, SQuADWeightMapper

# -------------------------------
# åŠ è½½å¹¶è½¬æ¢BERTæƒé‡ï¼ˆå°±æ˜¯ä½ ä¹‹å‰é‚£å¥—ä»£ç ï¼‰
@st.cache_resource
def load_model():
    # åŠ è½½é¢„è®­ç»ƒBERT
    pretrained_model = BertForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
    pretrained_state_dict = pretrained_model.state_dict()

    # è‡ªå®šä¹‰æ¨¡å‹
    my_bert = SQuADWeightMapper(vocab_size=30522, hidden=1024, n_layers=24, attn_heads=16)
    my_bert_state_dict = my_bert.state_dict()

    # åŠ è½½Embedding
    my_bert_state_dict["bert.embedding.token.weight"].copy_(pretrained_state_dict["bert.embeddings.word_embeddings.weight"])
    my_bert_state_dict["bert.embedding.position.pe"][0].copy_(pretrained_state_dict["bert.embeddings.position_embeddings.weight"])
    my_bert_state_dict["bert.embedding.segment.weight"][:2].copy_(pretrained_state_dict["bert.embeddings.token_type_embeddings.weight"])
    my_bert_state_dict["bert.embedding.norm.gamma"].copy_(pretrained_state_dict["bert.embeddings.LayerNorm.weight"])
    my_bert_state_dict["bert.embedding.norm.beta"].copy_(pretrained_state_dict["bert.embeddings.LayerNorm.bias"])

    # åŠ è½½Transformer
    for layer_idx in range(24):
        prefix_hf = f"bert.encoder.layer.{layer_idx}."
        prefix_my = f"bert.transformer_blocks.{layer_idx}."

        my_bert_state_dict[prefix_my + "attention.w_q.weight"].copy_(pretrained_state_dict[prefix_hf + "attention.self.query.weight"])
        my_bert_state_dict[prefix_my + "attention.w_q.bias"].copy_(pretrained_state_dict[prefix_hf + "attention.self.query.bias"])
        my_bert_state_dict[prefix_my + "attention.w_k.weight"].copy_(pretrained_state_dict[prefix_hf + "attention.self.key.weight"])
        my_bert_state_dict[prefix_my + "attention.w_k.bias"].copy_(pretrained_state_dict[prefix_hf + "attention.self.key.bias"])
        my_bert_state_dict[prefix_my + "attention.w_v.weight"].copy_(pretrained_state_dict[prefix_hf + "attention.self.value.weight"])
        my_bert_state_dict[prefix_my + "attention.w_v.bias"].copy_(pretrained_state_dict[prefix_hf + "attention.self.value.bias"])
        my_bert_state_dict[prefix_my + "attention.w_concat.weight"].copy_(pretrained_state_dict[prefix_hf + "attention.output.dense.weight"])
        my_bert_state_dict[prefix_my + "attention.w_concat.bias"].copy_(pretrained_state_dict[prefix_hf + "attention.output.dense.bias"])
        my_bert_state_dict[prefix_my + "norm1.gamma"].copy_(pretrained_state_dict[prefix_hf + "attention.output.LayerNorm.weight"])
        my_bert_state_dict[prefix_my + "norm1.beta"].copy_(pretrained_state_dict[prefix_hf + "attention.output.LayerNorm.bias"])
        my_bert_state_dict[prefix_my + "ffn.linear1.weight"].copy_(pretrained_state_dict[prefix_hf + "intermediate.dense.weight"])
        my_bert_state_dict[prefix_my + "ffn.linear1.bias"].copy_(pretrained_state_dict[prefix_hf + "intermediate.dense.bias"])
        my_bert_state_dict[prefix_my + "ffn.linear2.weight"].copy_(pretrained_state_dict[prefix_hf + "output.dense.weight"])
        my_bert_state_dict[prefix_my + "ffn.linear2.bias"].copy_(pretrained_state_dict[prefix_hf + "output.dense.bias"])
        my_bert_state_dict[prefix_my + "norm2.gamma"].copy_(pretrained_state_dict[prefix_hf + "output.LayerNorm.weight"])
        my_bert_state_dict[prefix_my + "norm2.beta"].copy_(pretrained_state_dict[prefix_hf + "output.LayerNorm.bias"])

    # åŠ è½½QA Head
    my_bert_state_dict["qa_outputs.weight"].copy_(pretrained_state_dict["qa_outputs.weight"])
    my_bert_state_dict["qa_outputs.bias"].copy_(pretrained_state_dict["qa_outputs.bias"])

    my_bert.eval()
    return my_bert

# -------------------------------
# åŠ è½½Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

# -------------------------------
# Streamlitç•Œé¢
st.title("ğŸ“š åŸºäºè‡ªåˆ¶BERTçš„SQuADé—®ç­”ç³»ç»Ÿ")

question = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜", "Where do I live?")
# é»˜è®¤æ–‡æœ¬
default_text = "My name is John and I live in New York."

# åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œè‡ªåŠ¨æ ¹æ®è¡Œæ•°æ¥è®¾ç½®æ˜¾ç¤ºçš„é«˜åº¦
context = st.text_area("è¯·è¾“å…¥ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰", default_text, height=300)

if st.button("è·å–ç­”æ¡ˆ"):
    if not question.strip() or not context.strip():
        st.warning("è¯·è¾“å…¥é—®é¢˜å’Œä¸Šä¸‹æ–‡")
    else:
        model = load_model()
        inputs = tokenizer(question, context, return_tensors="pt")
        input_ids = inputs["input_ids"]
        token_type_ids = inputs["token_type_ids"]

        with torch.no_grad():
            start_logits, end_logits = model(input_ids, token_type_ids)
            start = torch.argmax(start_logits)
            end = torch.argmax(end_logits) + 1
            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start:end]))

        st.success(f"ğŸ‰ ç­”æ¡ˆæ˜¯ï¼š{answer}")
