{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ca613a-7705-44d3-b6be-2bcea3024849",
   "metadata": {},
   "source": [
    "# [BERT](https://arxiv.org/pdf/1810.04805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de477d52-960c-465f-99f0-92cb70ebd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcf984-c671-4fa0-802a-a32ef7c9d034",
   "metadata": {},
   "source": [
    "## Attention\n",
    "**Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Multi-Head-Attention.png\" width=\"600\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f64ec54-a7e7-44b8-b55b-fd5f7d629101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    计算 '缩放点积注意力' (Scaled Dot Product Attention)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        # 计算注意力分数 (query 与 key 的点积，然后除以 sqrt(d_k) 进行缩放)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        # 如果有mask，屏蔽掉不该关注的位置（如padding）\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)  # 将mask为0的位置填充一个极小的值，softmax后接近于0\n",
    "\n",
    "        # 计算注意力权重 (对分数做softmax)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 如果提供了dropout，则对注意力权重进行dropout\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        # 返回加权后的value（注意力结果）和注意力权重\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11427496-3d9c-4038-85ba-e2373c43c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力机制\n",
    "    输入: 模型维度 (d_model) 和头数 (h)，将注意力机制拆分为多个头并行计算\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0  # 确保模型维度可以被头数整除\n",
    "\n",
    "        # 每个头的维度（假设d_k == d_v）\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        # 线性变换层，分别用于 query、key、value\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        \n",
    "        # 最终的输出线性变换\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 注意力计算模块（使用上面定义的Attention类）\n",
    "        self.attention = Attention()\n",
    "\n",
    "        # Dropout用于防止过拟合\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) 对 query、key 和 value 进行线性投影，并拆分成多头（batch_size, h, seq_len, d_k）\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) 计算注意力（每个头独立计算）\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) 将多头的结果拼接起来 (合并维度)，再通过输出线性层\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)  # 返回最终的输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86060faa-dafe-4de2-80fb-d7f7214389cf",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "**BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Embedding.png\" width=\"900\">\n",
    "</p>\n",
    "\n",
    "**Position Embeddings**：\n",
    "\n",
    "$$\\begin{aligned} P E_{(p o s, 2 i)} & =\\sin \\left(\\text { pos } / 10000^{2 i / d_{\\text {model }}}\\right) \\\\ P E_{(p o s, 2 i+1)} & =\\cos \\left(\\text { pos } / 10000^{2 i / d_{\\text {model }}}\\right)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f1b27b-e1ce-43c8-9201-51f7d032ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    位置编码模块（Positional Encoding）\n",
    "    通过正弦和余弦函数为每个位置生成唯一的编码，提供位置信息。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # 提前计算好位置编码矩阵，避免每次都重新计算\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False  # 不需要梯度更新（固定的）\n",
    "\n",
    "        # 生成位置索引（0 到 max_len - 1）\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "\n",
    "        # 计算每个维度对应的分母（按照论文公式，频率按 log 均匀分布）\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        # 偶数维度用 sin，奇数维度用 cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数位置\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数位置\n",
    "\n",
    "        # 增加batch维度（为了和输入x在batch维度对齐）\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 将位置编码注册为缓冲区，不作为参数更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 根据输入序列长度返回对应位置的编码\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fdae51-e64d-41eb-9623-4c2330704933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    句子片段编码（Segment Embedding）\n",
    "    用于区分句子A和句子B（例如：问答任务中问题和答案的区分），通常是 0, 1, 2 三种情况\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size=512):\n",
    "        # 3表示最多支持3个片段（一般只用到两个，0和1）\n",
    "        super().__init__(3, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fecbde3d-a9d2-4372-b1d7-28450a4af701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    词嵌入（Token Embedding）\n",
    "    将单词ID映射为稠密向量\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df11b2fc-2100-4b55-a4c2-5c274103a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT嵌入层（BERT Embedding）\n",
    "    \n",
    "    由以下三部分组成：\n",
    "      1. TokenEmbedding ：将词转换为向量\n",
    "      2. PositionalEmbedding ：为每个单词加入位置信息（让模型区分单词顺序）\n",
    "      3. SegmentEmbedding ：为句子片段加入片段信息（区分句子A和句子B）\n",
    "\n",
    "    这三者的加和作为BERT的最终输入，之后送入Transformer。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化BERTEmbedding\n",
    "        :param vocab_size: 词表大小\n",
    "        :param embed_size: 每个单词的向量维度（embedding size）\n",
    "        :param dropout: dropout概率，防止过拟合\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)     # 词嵌入\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)         # 位置嵌入\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)          # 片段嵌入\n",
    "        self.dropout = nn.Dropout(p=dropout)                                          # dropout\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param sequence: 输入的单词ID序列\n",
    "        :param segment_label: 输入的片段ID序列（例如全0或全1，标识属于哪一个句子）\n",
    "        :return: 融合了三种信息后的embedding（带位置信息和片段信息）\n",
    "        \"\"\"\n",
    "        # 将词嵌入、位置嵌入和片段嵌入相加\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "\n",
    "        # 最后加上dropout\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f9c20-d971-4059-b7c9-e63b6ac8e9fa",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c588ae-fb96-4f0a-bad3-730c43a827fd",
   "metadata": {},
   "source": [
    "**[GAUSSIAN ERROR LINEAR UNITS (GELUS)](https://arxiv.org/pdf/1606.08415)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/QuickGELU.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf93e7b2-cabe-48c2-a063-a6668a763dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    激活函数：GELU（高斯误差线性单元）\n",
    "    \n",
    "    来自论文的第 3.4 小节，BERT 选择了 GELU 而不是 ReLU 作为激活函数。\n",
    "    GELU 比 ReLU 更平滑，在自然语言任务中表现更好。\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算GELU公式，返回GELU激活后的结果\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d5794-b765-4615-8401-719a72c5e42e",
   "metadata": {},
   "source": [
    "[Layer Normalization](https://arxiv.org/pdf/1607.06450)\n",
    "\n",
    "$y =\\frac{ x - E ( x )}{\\sqrt{\\operatorname{Var}( x )+\\epsilon}} * \\gamma+\\beta$\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/layer_norm.png\" width=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08df318-46da-4b12-a9d7-76316e0e7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    层归一化（Layer Normalization）\n",
    "    \n",
    "    作用：对输入特征在最后一个维度上做归一化（保证均值为0，标准差为1），\n",
    "    并引入可学习的缩放和偏移参数，提升训练稳定性。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 可学习参数，a_2是缩放因子，b_2是偏移量\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps  # 避免除零的小常数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算均值和标准差（在最后一个维度上）\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "\n",
    "        # 标准化 + 缩放 + 偏移\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656c8320-2efc-4144-ab22-a0e86b650359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    子层连接（Sublayer Connection）\n",
    "    \n",
    "    结构: 残差连接（Residual） + 层归一化（LayerNorm）\n",
    "    \n",
    "    注意：与ResNet不同，这里是 \"先归一化再残差连接\"，以保持代码简单。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)           # 层归一化\n",
    "        self.dropout = nn.Dropout(dropout)    # dropout 防止过拟合\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        x -> 归一化 -> 传入子层（如注意力或前馈）-> dropout -> 残差加和\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af50e26-1418-4f23-8143-0454ea7e9cfd",
   "metadata": {},
   "source": [
    "**Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/PositionwiseFeedForward.png\" width=\"200\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860f7ec7-5799-4f2d-8225-9812edcd2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    前馈全连接层（Position-wise Feed Forward Network, FFN）\n",
    "\n",
    "    这个模块在Transformer中对每个位置的表示单独进行非线性变换：\n",
    "    公式：FFN(x) = max(0, x * W1 + b1) * W2 + b2 （这里激活函数是GELU而不是ReLU）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)   # 第一层全连接，将维度扩展到d_ff\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)   # 第二层全连接，将维度变回d_model\n",
    "        self.dropout = nn.Dropout(dropout)    # dropout层\n",
    "        self.activation = GELU()              # 使用GELU激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 依次通过w_1 -> GELU激活 -> dropout -> w_2\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b4c83-f471-4b97-b23c-2f062b283406",
   "metadata": {},
   "source": [
    "## Transformer Encode\n",
    "**Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/EncoderLayer.png\" width=\"250\">\n",
    "</p>\n",
    "但下面实现的是包含 Masked 机制的 Transformer Encoder 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291dc381-c13c-488c-8320-662e28c655bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    双向编码器（Transformer块）\n",
    "    \n",
    "    组成部分：\n",
    "    - 多头自注意力机制（Multi-Head Attention）\n",
    "    - 前馈全连接层（Feed Forward）\n",
    "    - 子层连接（残差 + LayerNorm）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        初始化 TransformerBlock\n",
    "\n",
    "        :param hidden: Transformer隐藏层维度\n",
    "        :param attn_heads: 多头注意力机制中的头数\n",
    "        :param feed_forward_hidden: 前馈全连接网络的隐藏层维度，一般是hidden的4倍\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 多头注意力机制\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "\n",
    "        # 前馈全连接网络\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "\n",
    "        # 两个子层连接（残差 + LayerNorm）\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "\n",
    "        # 最后加一个dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    # 注意：本实现中的 Transformer Encoder 块集成 Masked 机制\n",
    "    def forward(self, x, mask):\n",
    "        # 第一步：自注意力 + 残差 + LayerNorm\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "\n",
    "        # 第二步：前馈网络 + 残差 + LayerNorm\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "\n",
    "        # 最后做一次dropout\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a192d0-372e-4b6e-aed7-94aaeae56a1e",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ea7288-9640-4dac-9163-49713cb604f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT 模型：双向编码器表示（Bidirectional Encoder Representations from Transformers）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化BERT\n",
    "\n",
    "        :param vocab_size: 词表大小\n",
    "        :param hidden: 隐藏层维度（一般为768）\n",
    "        :param n_layers: Transformer块的数量（即深度）\n",
    "        :param attn_heads: 多头注意力的头数\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # 前馈层的隐藏层大小，通常是hidden的4倍\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "\n",
    "        # BERT的嵌入层（Token + Positional + Segment Embedding之和）\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n",
    "\n",
    "        # 多层Transformer块（堆叠n_layers层）\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # 创建注意力mask，pad位置为False，其他为True（防止pad影响注意力计算）\n",
    "        # mask维度: [batch_size, 1, seq_len, seq_len]\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # 输入x经过嵌入层（得到词向量 + 位置向量 + segment向量的和）\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # 依次通过多个Transformer块\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        # 最终输出\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c988a-4bb5-41a0-ad7e-d883991f2e5a",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "**BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/BERT-Pre-training-Fine-Tuning.png\" width=\"900\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f61b47-1c7e-48ff-aa96-5ee77ff58f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    下一句预测分类器（2分类：is_next 或 is_not_next）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT输出的隐藏层维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)  # 线性层，输出两个类别\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)  # LogSoftmax用于计算对数概率\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 取BERT输出的第一个token（[CLS]）做分类\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98ffa6f-e1ee-4b57-a776-b0cd51252d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    掩码语言模型（Masked Language Model）\n",
    "\n",
    "    任务：预测被mask掉的token，属于多分类问题，类别数等于词表大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT输出的隐藏层维度\n",
    "        :param vocab_size: 词表大小\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)  # 线性层，输出词表大小的logits\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)  # LogSoftmax \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 直接输出每个token的分类结果（词表概率分布）\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "174be20a-92f2-4cb3-b000-bd18976377f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT语言模型（BERTLM）\n",
    "\n",
    "    由两部分组成：\n",
    "    - Next Sentence Prediction（下一句预测）\n",
    "    - Masked Language Model（掩码语言模型）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: 已训练的BERT模型\n",
    "        :param vocab_size: 词表大小（用于Masked LM预测）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "        # 下一句预测分类器\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "\n",
    "        # 掩码语言模型分类器\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        # 经过BERT主模型\n",
    "        x = self.bert(x, segment_label)\n",
    "\n",
    "        # 返回下一句预测和掩码语言模型的输出\n",
    "        return self.next_sentence(x), self.mask_lm(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
