{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59c90a6-078b-4d3a-906f-4fa6a4a9d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8db341d9-8016-46da-9c91-85639664f71d",
   "metadata": {},
   "source": [
    "## Bottleneck Model\n",
    "**ResNet: [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Resnet-Bottleneck.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828fbc8b-8f57-4926-9ce4-2e467f5e1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Bottleneck 模块，基本残差单元，使用了 ResNet 的 Bottleneck 架构\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4  # 输出通道数会被扩展 4 倍\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 第一层 1x1 卷积用于压缩通道数\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 第二层 3x3 卷积保持通道数\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 如果 stride > 1，使用 avgpool 替代卷积下采样（抗锯齿）\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        # 第三层 1x1 卷积扩展通道数\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        # 如果维度不一致或需要下采样，定义跳连分支\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),  # 使用平均池化代替 stride 卷积\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)  # 只有当 stride > 1 时才真正执行下采样\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity  # 残差连接\n",
    "        out = self.relu3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408470b-c612-4ba7-bf69-4d096896c679",
   "metadata": {},
   "source": [
    "## AttentionPool2d Model\n",
    "**Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Multi-Head-Attention.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0c4819-22c4-4d01-9db2-b24d7a683ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttentionPool2d：使用注意力代替平均池化的模块（视觉Transformer常用）\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int=None):\n",
    "        super().__init__()\n",
    "        # 添加位置编码\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        # 三个线性变换用于 Q/K/V\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        # 输出投影\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # 将输入展平为 (H*W, N, C)，再 permute 成 transformer 所需格式，N为Batch Size\n",
    "        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # 增加 [CLS] token\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)\n",
    "\n",
    "        # Transformer\n",
    "        query, key, value = x[:1], x, x\n",
    "        tgt_len, bsz, embed_dim = query.shape\n",
    "        src_len, _, _ = key.shape\n",
    "        \n",
    "        # 投影到 Q、K、V，Linear\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        # 变形，准备多头\n",
    "        q = q.view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)  # (bsz*num_heads, tgt_len, head_dim)\n",
    "        k = k.view(src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)  # (bsz*num_heads, src_len, head_dim)\n",
    "        v = v.view(src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)  # (bsz*num_heads, src_len, head_dim)\n",
    "\n",
    "        # 重新调整为(batch_size, num_heads, seq_len, head_dim)格式\n",
    "        q = q.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        k = k.view(bsz, self.num_heads, src_len, self.head_dim)\n",
    "        v = v.view(bsz, self.num_heads, src_len, self.head_dim)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        # MatMul + Scale\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Mask(.opt)\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, tgt_len, src_len)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(1)  # (batch_size, 1, tgt_len, src_len)\n",
    "        attn_scores = attn_scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        # MatMul\n",
    "        attn_output = torch.matmul(attn_probs, v)  # (bsz, num_heads, tgt_len, head_dim)\n",
    "\n",
    "        # Concat\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(tgt_len * bsz, self.embed_dim)\n",
    "        # Linear\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_output.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eac88c-d116-4a4f-a908-13bb1061ed9e",
   "metadata": {},
   "source": [
    "## ModifiedResNet\n",
    "**[Designing Network Design Spaces](https://arxiv.org/abs/1706.03762)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/stem.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94dd97b1-dcf7-4983-859e-e1add07e9443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified ResNet 架构，包含以下不同之处：\n",
    "    - 使用3层卷积替代传统 ResNet 的1个 stem 卷积（更细致特征提取）\n",
    "    - 所有下采样都使用 avgpool + conv 实现（抗锯齿设计）\n",
    "    - 最终使用 AttentionPool2d 替代全局平均池化（更强表达能力）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # 三层stem卷积\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(2)  # 下采样\n",
    "\n",
    "        # 残差层构建\n",
    "        self._inplanes = width\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # 输出特征维度\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem部分\n",
    "        def stem(x):\n",
    "            x = self.relu1(self.bn1(self.conv1(x)))\n",
    "            x = self.relu2(self.bn2(self.conv2(x)))\n",
    "            x = self.relu3(self.bn3(self.conv3(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)  # 匹配输入类型（float16/float32）\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)  # 用注意力池化取代平均池化\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6a5fe-1814-47b7-8319-e99c75ca1062",
   "metadata": {},
   "source": [
    "## QuickGELU\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/QuickGELU.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0a11c4-ea18-425b-a868-46b1a84cbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 使用 QuickGELU 激活函数，比标准 GELU 更快\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7257508e-1649-4256-88eb-7c5e80b88ba4",
   "metadata": {},
   "source": [
    "## ResidualAttentionBlock\n",
    "**[Vision Transformer(Vit)](https://arxiv.org/pdf/2010.11929)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Transformer-Encoder.png\" width=\"200\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88bc0fa5-2e75-4f0c-8720-e129cf4afd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"继承自 PyTorch 的 LayerNorm，增加对 fp16（半精度）输入的支持\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype  # 记录原始数据类型\n",
    "        ret = super().forward(x.type(torch.float32))  # 先将输入转换为 float32 进行归一化\n",
    "        return ret.type(orig_type)  # 再转换回原始数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8700f64b-b508-4fd6-a1ec-3b5a19541643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # 多头自注意力模块\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "\n",
    "        # LayerNorm + MLP\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),  # 全连接层，升维\n",
    "            (\"gelu\", QuickGELU()),  # QuickGELU 激活函数\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))  # 全连接层，降维回原大小\n",
    "        ]))\n",
    "\n",
    "        # 注意力掩码（可选）\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        # 如果有注意力掩码，则将其转换为与输入相同的 dtype 和设备\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        # 执行自注意力计算（只取输出，不需要权重）\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    # Vision Transformer中的Transformer Encoder\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 残差连接 + 注意力\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        # 残差连接 + MLP\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ef0ec-8959-46d3-9491-e99d68d33070",
   "metadata": {},
   "source": [
    "## VisionTransformer\n",
    "**[Vision Transformer(Vit)](https://arxiv.org/pdf/2010.11929)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Vit.png\" width=\"700\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbdb3cc4-39ba-4f25-854e-6e7eab91a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        # 构建多个 ResidualAttentionBlock 组成的 Transformer 层\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)  # 逐层传递输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35a4be2-5c19-4090-bf9c-40d1daa7a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # 图像分块并进行线性投影，相当于 patch embedding\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "        # 初始化类标记向量和位置嵌入\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))  # 类别嵌入（分类用）\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))  # 位置编码\n",
    "\n",
    "        self.ln_pre = LayerNorm(width)  # Transformer 输入前的 LayerNorm\n",
    "\n",
    "        # Transformer 编码器\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        # 输出处理层：LayerNorm + 线性映射\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))  # 最后的输出投影矩阵\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 将图像切分为 patch 并做线性投影\n",
    "        x = self.conv1(x)  # 输出形状: [batch, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # 展平空间维度: [batch, width, grid**2]\n",
    "        x = x.permute(0, 2, 1)  # 交换维度: [batch, grid**2, width]\n",
    "\n",
    "        # 添加类别嵌入（class token）\n",
    "        class_token = self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([class_token, x], dim=1)  # 拼接类别标记: [batch, grid**2 + 1, width]\n",
    "\n",
    "        # 加上位置编码\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)  # 归一化\n",
    "\n",
    "        # 输入 transformer，注意 transformer 期望的输入是 [sequence_length, batch, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)  # [batch, seq_len, dim] -> [seq_len, batch, dim]\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, batch, dim] -> [batch, seq_len, dim]\n",
    "\n",
    "        # 取第一个 token（类别标记）作为输出，并做最后处理\n",
    "        x = self.ln_post(x[:, 0, :])  # 取出类别标记并归一化\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj  # 投影到输出维度\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1bda8-4df5-4e48-905e-e8c810884c8e",
   "metadata": {},
   "source": [
    "## Clip\n",
    "**[Clip](https://arxiv.org/pdf/2103.00020)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/clip.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76b106b-faef-4384-9a23-1a6e42c7e2de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mCLIP\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                 \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# 图像相关参数\u001b[39;49;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mtransformer_layers\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mCLIP\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCLIP\u001b[39;00m(nn.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m      3\u001b[39m                  embed_dim: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m      4\u001b[39m                  \u001b[38;5;66;03m# 图像相关参数\u001b[39;00m\n\u001b[32m      5\u001b[39m                  image_resolution: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m                  vision_layers: \u001b[43mUnion\u001b[49m[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mint\u001b[39m],\n\u001b[32m      7\u001b[39m                  vision_width: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m      8\u001b[39m                  vision_patch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m      9\u001b[39m                  \u001b[38;5;66;03m# 文本相关参数\u001b[39;00m\n\u001b[32m     10\u001b[39m                  context_length: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     11\u001b[39m                  vocab_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     12\u001b[39m                  transformer_width: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     13\u001b[39m                  transformer_heads: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     14\u001b[39m                  transformer_layers: \u001b[38;5;28mint\u001b[39m\n\u001b[32m     15\u001b[39m                  ):\n\u001b[32m     16\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     18\u001b[39m         \u001b[38;5;28mself\u001b[39m.context_length = context_length  \u001b[38;5;66;03m# 文本最大长度\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Union' is not defined"
     ]
    }
   ],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # 图像相关参数\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # 文本相关参数\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length  # 文本最大长度\n",
    "\n",
    "        # 根据传入参数选择视觉模型结构（ResNet 或 ViT）\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            # 如果是 ResNet，计算注意力头数量\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            # 如果是 Vision Transformer\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim\n",
    "            )\n",
    "\n",
    "        # 初始化文本 transformer 编码器\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()  # 构建自回归注意力掩码\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)  # 词嵌入矩阵\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))  # 位置嵌入\n",
    "        self.ln_final = LayerNorm(transformer_width)  # transformer 输出的最终 LayerNorm\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))  # 文本特征映射到共同空间\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # 特征对比的 logit 缩放参数\n",
    "\n",
    "        self.initialize_parameters()  # 初始化参数\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # 初始化词嵌入和位置嵌入\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        # 如果是 ResNet 并且使用了注意力池化，初始化注意力权重\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            # 将 ResNet 中残差块的第 3 层 BN 层的 gamma 参数初始化为 0\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        # 初始化 transformer 层的注意力和前馈网络权重\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        # 初始化文本投影矩阵\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # 构建一个因果注意力掩码，只允许关注当前及之前的 token\n",
    "        # PyTorch 使用加性 attention mask，这里填充为 -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # 上三角填 -inf，保持下三角为 0\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        # 返回模型中卷积层使用的张量数据类型（通常为 float32 或 float16）\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        # 对图像进行编码，输出图像特征\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        # 文本输入：[batch_size, context_length]\n",
    "        x = self.token_embedding(text).type(self.dtype)  # 获取 token 嵌入向量\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)  # 加上位置嵌入\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND，适配 transformer 输入\n",
    "        x = self.transformer(x)  # transformer 编码\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD，恢复原顺序\n",
    "        x = self.ln_final(x).type(self.dtype)  # 最后再归一化\n",
    "\n",
    "        # 获取每个样本中 End-of-Text（EOT）标记对应的特征作为整体文本表示\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # 编码图像和文本\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # 对图像和文本特征进行归一化\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # 计算图文之间的余弦相似度 logits（乘以 logit 缩放因子）\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # 返回图文相似度矩阵（图像对文本 / 文本对图像）\n",
    "        return logits_per_image, logits_per_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
