{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f4b7ea-f82d-487e-96e2-379e6b130791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81404612-44ca-407f-a937-63450ee7fdc0",
   "metadata": {},
   "source": [
    "## Transformer Encoder\n",
    "**Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Multi-Head-Attention.png\" width=\"750\">\n",
    "    <img src=\"./assets/Transformer-Encoder.png\" width=\"200\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a970947a-3b9e-444b-a137-878ab75596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm + Multi-Head Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5  # 缩放因子，防止点积过大\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)  # 层归一化，规范化输入\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)  # 计算注意力权重\n",
    "        self.dropout = nn.Dropout(dropout)  # dropout 防止过拟合\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)  # 一次性生成Q、K、V\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),  # 将多头输出拼接后的结果映射回原维度\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()  # 如果只有一个头并且维度不变，直接返回\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Norm\n",
    "        x = self.norm(x)  # 输入归一化，提升训练稳定性\n",
    "\n",
    "        # Linear\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)  # 拆分为 Q、K、V 三个部分\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)  \n",
    "        # 重排shape，方便做多头计算\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # QK转置后点乘，计算相似度\n",
    "        attn = self.attend(dots)  # softmax 获取注意力权重\n",
    "        attn = self.dropout(attn)  # dropout\n",
    "\n",
    "        out = torch.matmul(attn, v)  # 加权求和，获得注意力输出\n",
    "        \n",
    "        # concat\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')  # 将多头结果拼接回原shape\n",
    "        return self.to_out(out)  # 映射回原维度并返回\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99908264-1ffd-4904-be70-c42ce943563b",
   "metadata": {},
   "source": [
    "**[GAUSSIAN ERROR LINEAR UNITS (GELUS)](https://arxiv.org/pdf/1606.08415)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/QuickGELU.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd1d39d-b922-4378-967d-537aa041fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal + MLP\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),           # 层归一化，提升训练稳定性\n",
    "            # MLP \n",
    "            nn.Linear(dim, hidden_dim),  # 线性升维\n",
    "            nn.GELU(),                   # GELU 激活函数，非线性变换\n",
    "            nn.Dropout(dropout),         # dropout 防止过拟合\n",
    "            nn.Linear(hidden_dim, dim),  # 线性降维，回到原始维度\n",
    "            nn.Dropout(dropout)          # 再次 dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # 前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc141ffc-f3db-466d-88d8-6baa6d39c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encode\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)  # Transformer 最后的输出归一化\n",
    "        self.layers = nn.ModuleList([])  # 存储每一层的Attention和FeedForward\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),  # 多头自注意力\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)  # 前馈神经网络\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x   # 残差连接 + 注意力\n",
    "            x = ff(x) + x     # 残差连接 + 前馈网络\n",
    "\n",
    "        return self.norm(x)  # 最终归一化输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a1a16-92dd-45a2-a1a9-59eeb43941ea",
   "metadata": {},
   "source": [
    "## Vision Transformer\n",
    "**[Vision Transformer(Vit)](https://arxiv.org/pdf/2010.11929)**\n",
    "<p align=\"center\">\n",
    "    <img src=\"./assets/Vit.png\" width=\"700\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481375d-d1af-427a-91df-af76741a01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformer\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
    "        \"\"\"\n",
    "        Vision Transformer (ViT) 模型\n",
    "\n",
    "        参数说明:\n",
    "        image_size: 输入图片大小 (可以是单个整数或tuple)\n",
    "        patch_size: 每个 patch 的大小 (可以是单个整数或tuple)\n",
    "        num_classes: 分类任务的类别数量\n",
    "        dim: patch 嵌入的维度\n",
    "        depth: Transformer 的层数\n",
    "        heads: 多头自注意力中的头数\n",
    "        mlp_dim: Transformer中MLP层的隐藏维度\n",
    "        pool: 'cls' 或 'mean'，表示池化方式\n",
    "        channels: 输入图像的通道数（默认是3，即RGB图像）\n",
    "        dim_head: 每个注意力头的维度\n",
    "        dropout: Transformer中的dropout率\n",
    "        emb_dropout: patch嵌入后的dropout率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 解析图像大小和patch大小，保证为 (height, width) 形式\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        # 确保图像尺寸能够被patch大小整除\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        # 计算patch的数量\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "\n",
    "        # 每个patch展平成一维后的长度\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        # pool方式检查\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        # patch embedding 模块，将图像分割成patch并进行线性映射\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # 使用einops重排，将输入图像划分为patch\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "            \n",
    "            # 对每个patch进行LayerNorm标准化\n",
    "            nn.LayerNorm(patch_dim),\n",
    "\n",
    "            # 线性映射到指定维度\n",
    "            nn.Linear(patch_dim, dim),\n",
    "\n",
    "            # 再次进行LayerNorm\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        # 位置编码 + [CLS] token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))  # +1是为cls token预留的\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))  # 分类用的CLS token\n",
    "        self.dropout = nn.Dropout(emb_dropout)  # 嵌入后的Dropout\n",
    "\n",
    "        # Transformer 编码器\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool  # 池化方式 ('cls' 或 'mean')\n",
    "        self.to_latent = nn.Identity()  # 占位层，保持x不变\n",
    "\n",
    "        # 最后的分类MLP头\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \"\"\"\n",
    "        # Linear Projection of Flattened Patches\n",
    "        # 将输入图像划分为patch并嵌入\n",
    "        x = self.to_patch_embedding(img)  # [batch_size, num_patches, dim]\n",
    "\n",
    "        b, n, _ = x.shape  # 获取批次大小和patch数量\n",
    "\n",
    "         # Patch + Position + Embedding\n",
    "        # 添加分类token（cls token），并拼接到patch序列的最前面\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)  # [batch_size, 1, dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [batch_size, num_patches + 1, dim]\n",
    "\n",
    "        # 添加位置编码\n",
    "        x += self.pos_embedding[:, :(n + 1)]  # [batch_size, num_patches + 1, dim]\n",
    "        x = self.dropout(x)  # 植入dropout防止过拟合\n",
    "\n",
    "        # Transformer Encode\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # 选择池化方式\n",
    "        if self.pool == 'mean':\n",
    "            x = x.mean(dim=1)  # 对所有token（包括cls token）取平均\n",
    "        else:\n",
    "            x = x[:, 0]  # 只取cls token\n",
    "\n",
    "        x = self.to_latent(x)  # 这里为Identity，保持x不变\n",
    "\n",
    "        # MLP Head\n",
    "        # 分类输出\n",
    "        return self.mlp_head(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
