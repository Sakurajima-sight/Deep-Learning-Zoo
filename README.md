# üß† Deep-Learning-Zoo

Welcome to the **Deep Learning Zoo** ‚Äî  
a curated, hands-on collection of deep neural architectures I‚Äôve studied, dissected, and experimented with.

ü¶Å Whether it‚Äôs a vision beast, a language dragon, or an audio chimera, every model has its own cage in this zoo.

## üìö Purpose

- To systematically organize all models I‚Äôve seriously explored and reproduced  
- To provide explanations, paper links, visualizations, and clean implementations  
- To build a long-term neural model knowledge base for future reference and growth  

## ü¶é Multimodal 

| Model Architecture | GitHub Repository (Based on)                      | Paper                                                        |
| ------------------ | ------------------------------------------------- | ------------------------------------------------------------ |
| CLIP               | **[openai/CLIP](https://github.com/openai/CLIP)** | **[Learning Transferable Visual Models from Natural Language Supervision](https://arxiv.org/abs/2103.00020)** |

## üñºÔ∏è Computer Vision

| Model Architecture | GitHub Repository (Based on) | Paper |
| ------------------ | ----------------- | ----- |
| ViT (Vision Transformer) | **[vit-pytorch](https://github.com/lucidrains/vit-pytorch)** | **[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)** |

## üìö Natural Language Processing

| Model Architecture                                           | GitHub Repository (Based on)                                 | Paper                                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Transformer (Attention Is All You Need)                      | **[transformer](https://github.com/hyunwoongko/transformer)** | **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** |
| BERT (Bidirectional Encoder Representations from Transformers) | **[BERT-pytorch](https://github.com/codertimo/BERT-pytorch)** | **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)** |


## üîä Audio

## üß† Reinforcement Learning 

## üß™ Other